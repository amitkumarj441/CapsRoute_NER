{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ith/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "from keras import callbacks, optimizers\n",
    "from keras import backend as KB\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Reshape\n",
    "from keras.layers import Bidirectional, GRU, Flatten, SpatialDropout1D, Conv1D\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from common import vocabulary, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.8.0\n",
      "Keras version: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"Keras version:\", K.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First attempt\n",
    "x_train: sentence - list of words  \n",
    "y_train: sentence - list of NER tag\n",
    "\n",
    "### Second attempt (maybe?)\n",
    "x_train: sentence - list of words, POS tags  \n",
    "y_train: sentence - list of NER tags\n",
    "\n",
    "### Notes\n",
    "> Does it makes sense to use 'O' as the padding for the NER tags? Should we use something else?\n",
    "> - Currently, I am using `<s>` and `</s>`  \n",
    "\n",
    "> perhaps we want to try without the I-, B- modifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"./data/conll2003/eng.train\"\n",
    "DEV_FILE = \"./data/conll2003/eng.testa\"\n",
    "TEST_FILE = \"./data/conll2003/eng.testb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-DOCSTART- -X- -X- O\r\n",
      "\r\n",
      "EU NNP I-NP I-ORG\r\n",
      "rejects VBZ I-VP O\r\n",
      "German JJ I-NP I-MISC\r\n",
      "call NN I-NP O\r\n",
      "to TO I-VP O\r\n",
      "boycott VB I-VP O\r\n",
      "British JJ I-NP I-MISC\r\n",
      "lamb NN I-NP O\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 {TRAIN_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is odd that the B-PER tag doesn't appear... investigate this.\n",
    "!grep \"B-PER\" {TRAIN_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile( filename, pos=False):\n",
    "    '''\n",
    "    read the conll2003 file\n",
    "    \n",
    "    filename(string) - path to conll2003 file (train, test, etc.)\n",
    "    pos(boolean) - flag if true will include pos tags in returned list\n",
    "    returns a list of lists of lists corresponding to the words in each sentence\n",
    "    \n",
    "    '''\n",
    "    f = open(filename)\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in f:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.strip().split(' ')\n",
    "        word = [splits[0], splits[1], splits[-1]] if pos else [splits[0], splits[-1]]\n",
    "        sentence.append( word)\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        sentences.append(sentence)\n",
    "        sentence = []\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences = readfile(TRAIN_FILE, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['EU', 'NNP', 'I-ORG'], ['rejects', 'VBZ', 'O'], ['German', 'JJ', 'I-MISC'], ['call', 'NN', 'O'], ['to', 'TO', 'O'], ['boycott', 'VB', 'O'], ['British', 'JJ', 'I-MISC'], ['lamb', 'NN', 'O'], ['.', '.', 'O']]\n",
      "[['Peter', 'NNP', 'I-PER'], ['Blackburn', 'NNP', 'I-PER']]\n",
      "[['BRUSSELS', 'NNP', 'I-LOC'], ['1996-08-22', 'CD', 'O']]\n",
      "[['The', 'DT', 'O'], ['European', 'NNP', 'I-ORG'], ['Commission', 'NNP', 'I-ORG'], ['said', 'VBD', 'O'], ['on', 'IN', 'O'], ['Thursday', 'NNP', 'O'], ['it', 'PRP', 'O'], ['disagreed', 'VBD', 'O'], ['with', 'IN', 'O'], ['German', 'JJ', 'I-MISC'], ['advice', 'NN', 'O'], ['to', 'TO', 'O'], ['consumers', 'NNS', 'O'], ['to', 'TO', 'O'], ['shun', 'VB', 'O'], ['British', 'JJ', 'I-MISC'], ['lamb', 'NN', 'O'], ['until', 'IN', 'O'], ['scientists', 'NNS', 'O'], ['determine', 'VBP', 'O'], ['whether', 'IN', 'O'], ['mad', 'JJ', 'O'], ['cow', 'NN', 'O'], ['disease', 'NN', 'O'], ['can', 'MD', 'O'], ['be', 'VB', 'O'], ['transmitted', 'VBN', 'O'], ['to', 'TO', 'O'], ['sheep', 'NN', 'O'], ['.', '.', 'O']]\n",
      "[['Germany', 'NNP', 'I-LOC'], [\"'s\", 'POS', 'O'], ['representative', 'NN', 'O'], ['to', 'TO', 'O'], ['the', 'DT', 'O'], ['European', 'NNP', 'I-ORG'], ['Union', 'NNP', 'I-ORG'], [\"'s\", 'POS', 'O'], ['veterinary', 'JJ', 'O'], ['committee', 'NN', 'O'], ['Werner', 'NNP', 'I-PER'], ['Zwingmann', 'NNP', 'I-PER'], ['said', 'VBD', 'O'], ['on', 'IN', 'O'], ['Wednesday', 'NNP', 'O'], ['consumers', 'NNS', 'O'], ['should', 'MD', 'O'], ['buy', 'VB', 'O'], ['sheepmeat', 'NN', 'O'], ['from', 'IN', 'O'], ['countries', 'NNS', 'O'], ['other', 'JJ', 'O'], ['than', 'IN', 'O'], ['Britain', 'NNP', 'I-LOC'], ['until', 'IN', 'O'], ['the', 'DT', 'O'], ['scientific', 'JJ', 'O'], ['advice', 'NN', 'O'], ['was', 'VBD', 'O'], ['clearer', 'JJR', 'O'], ['.', '.', 'O']]\n",
      "[['\"', '\"', 'O'], ['We', 'PRP', 'O'], ['do', 'VBP', 'O'], [\"n't\", 'RB', 'O'], ['support', 'VB', 'O'], ['any', 'DT', 'O'], ['such', 'JJ', 'O'], ['recommendation', 'NN', 'O'], ['because', 'IN', 'O'], ['we', 'PRP', 'O'], ['do', 'VBP', 'O'], [\"n't\", 'RB', 'O'], ['see', 'VB', 'O'], ['any', 'DT', 'O'], ['grounds', 'NNS', 'O'], ['for', 'IN', 'O'], ['it', 'PRP', 'O'], [',', ',', 'O'], ['\"', '\"', 'O'], ['the', 'DT', 'O'], ['Commission', 'NNP', 'I-ORG'], [\"'s\", 'POS', 'O'], ['chief', 'JJ', 'O'], ['spokesman', 'NN', 'O'], ['Nikolaus', 'NNP', 'I-PER'], ['van', 'NNP', 'I-PER'], ['der', 'FW', 'I-PER'], ['Pas', 'NNP', 'I-PER'], ['told', 'VBD', 'O'], ['a', 'DT', 'O'], ['news', 'NN', 'O'], ['briefing', 'NN', 'O'], ['.', '.', 'O']]\n",
      "[['He', 'PRP', 'O'], ['said', 'VBD', 'O'], ['further', 'JJ', 'O'], ['scientific', 'JJ', 'O'], ['study', 'NN', 'O'], ['was', 'VBD', 'O'], ['required', 'VBN', 'O'], ['and', 'CC', 'O'], ['if', 'IN', 'O'], ['it', 'PRP', 'O'], ['was', 'VBD', 'O'], ['found', 'VBN', 'O'], ['that', 'IN', 'O'], ['action', 'NN', 'O'], ['was', 'VBD', 'O'], ['needed', 'VBN', 'O'], ['it', 'PRP', 'O'], ['should', 'MD', 'O'], ['be', 'VB', 'O'], ['taken', 'VBN', 'O'], ['by', 'IN', 'O'], ['the', 'DT', 'O'], ['European', 'NNP', 'I-ORG'], ['Union', 'NNP', 'I-ORG'], ['.', '.', 'O']]\n",
      "[['He', 'PRP', 'O'], ['said', 'VBD', 'O'], ['a', 'DT', 'O'], ['proposal', 'NN', 'O'], ['last', 'JJ', 'O'], ['month', 'NN', 'O'], ['by', 'IN', 'O'], ['EU', 'NNP', 'I-ORG'], ['Farm', 'NNP', 'O'], ['Commissioner', 'NNP', 'O'], ['Franz', 'NNP', 'I-PER'], ['Fischler', 'NNP', 'I-PER'], ['to', 'TO', 'O'], ['ban', 'VB', 'O'], ['sheep', 'NN', 'O'], ['brains', 'NNS', 'O'], [',', ',', 'O'], ['spleens', 'NNS', 'O'], ['and', 'CC', 'O'], ['spinal', 'JJ', 'O'], ['cords', 'NNS', 'O'], ['from', 'IN', 'O'], ['the', 'DT', 'O'], ['human', 'NN', 'O'], ['and', 'CC', 'O'], ['animal', 'NN', 'O'], ['food', 'NN', 'O'], ['chains', 'NNS', 'O'], ['was', 'VBD', 'O'], ['a', 'DT', 'O'], ['highly', 'RB', 'O'], ['specific', 'JJ', 'O'], ['and', 'CC', 'O'], ['precautionary', 'JJ', 'O'], ['move', 'NN', 'O'], ['to', 'TO', 'O'], ['protect', 'VB', 'O'], ['human', 'JJ', 'O'], ['health', 'NN', 'O'], ['.', '.', 'O']]\n",
      "[['Fischler', 'JJR', 'I-PER'], ['proposed', 'VBN', 'O'], ['EU-wide', 'NNP', 'I-MISC'], ['measures', 'VBZ', 'O'], ['after', 'IN', 'O'], ['reports', 'NNS', 'O'], ['from', 'IN', 'O'], ['Britain', 'NNP', 'I-LOC'], ['and', 'CC', 'O'], ['France', 'NNP', 'I-LOC'], ['that', 'WDT', 'O'], ['under', 'IN', 'O'], ['laboratory', 'NN', 'O'], ['conditions', 'NNS', 'O'], ['sheep', 'NN', 'O'], ['could', 'MD', 'O'], ['contract', 'VB', 'O'], ['Bovine', 'NNP', 'I-MISC'], ['Spongiform', 'NNP', 'I-MISC'], ['Encephalopathy', 'NNP', 'I-MISC'], ['(', '(', 'O'], ['BSE', 'NNP', 'I-MISC'], [')', ')', 'O'], ['--', ':', 'O'], ['mad', 'JJ', 'O'], ['cow', 'NN', 'O'], ['disease', 'NN', 'O'], ['.', '.', 'O']]\n",
      "[['But', 'CC', 'O'], ['Fischler', 'NNP', 'I-PER'], ['agreed', 'VBD', 'O'], ['to', 'TO', 'O'], ['review', 'VB', 'O'], ['his', 'PRP$', 'O'], ['proposal', 'NN', 'O'], ['after', 'IN', 'O'], ['the', 'DT', 'O'], ['EU', 'NNP', 'I-ORG'], [\"'s\", 'POS', 'O'], ['standing', 'NN', 'O'], ['veterinary', 'JJ', 'O'], ['committee', 'NN', 'O'], [',', ',', 'O'], ['mational', 'JJ', 'O'], ['animal', 'NN', 'O'], ['health', 'NN', 'O'], ['officials', 'NNS', 'O'], [',', ',', 'O'], ['questioned', 'VBD', 'O'], ['if', 'IN', 'O'], ['such', 'JJ', 'O'], ['action', 'NN', 'O'], ['was', 'VBD', 'O'], ['justified', 'VBN', 'O'], ['as', 'IN', 'O'], ['there', 'RB', 'O'], ['was', 'VBD', 'O'], ['only', 'RB', 'O'], ['a', 'DT', 'O'], ['slight', 'JJ', 'O'], ['risk', 'NN', 'O'], ['to', 'TO', 'O'], ['human', 'JJ', 'O'], ['health', 'NN', 'O'], ['.', '.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    print(trainSentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the max sentence length\n",
    "> Let's start by clipping the longest 5%  \n",
    "> This will probably change, but we've got to start somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLens = np.array([ len(s) for s in trainSentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5284644278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "clipPct = 5\n",
    "\n",
    "maxLen = np.percentile(trainLens, 100 - clipPct)\n",
    "histLens = plt.hist(trainLens, bins=30)\n",
    "plt.vlines( maxLen, 0, max(histLens[0]), linestyles=\"dashed\")\n",
    "plt.title(\"Sentence lengths in corpus\")\n",
    "plt.hist(trainLens, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences longer than 37.0 words: 5 %\n"
     ]
    }
   ],
   "source": [
    "print( \"Sentences longer than\", maxLen, \"words:\", clipPct, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([2.665e+03, 3.336e+03, 2.128e+03, 9.260e+02, 8.610e+02, 8.540e+02,\n",
      "       8.580e+02, 6.270e+02, 7.230e+02, 4.590e+02, 3.250e+02, 1.370e+02,\n",
      "       7.900e+01, 3.900e+01, 1.000e+01, 9.000e+00, 2.000e+00, 1.000e+00,\n",
      "       0.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
      "       0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e+00]), array([  1.        ,   4.73333333,   8.46666667,  12.2       ,\n",
      "        15.93333333,  19.66666667,  23.4       ,  27.13333333,\n",
      "        30.86666667,  34.6       ,  38.33333333,  42.06666667,\n",
      "        45.8       ,  49.53333333,  53.26666667,  57.        ,\n",
      "        60.73333333,  64.46666667,  68.2       ,  71.93333333,\n",
      "        75.66666667,  79.4       ,  83.13333333,  86.86666667,\n",
      "        90.6       ,  94.33333333,  98.06666667, 101.8       ,\n",
      "       105.53333333, 109.26666667, 113.        ]), <a list of 30 Patch objects>)\n",
      "Max sentence length:  113\n"
     ]
    }
   ],
   "source": [
    "print(histLens)\n",
    "print( \"Max sentence length: \", max(trainLens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['EU', 'NNP', 'I-ORG'],\n",
       "  ['rejects', 'VBZ', 'O'],\n",
       "  ['German', 'JJ', 'I-MISC'],\n",
       "  ['call', 'NN', 'O'],\n",
       "  ['to', 'TO', 'O'],\n",
       "  ['boycott', 'VB', 'O'],\n",
       "  ['British', 'JJ', 'I-MISC'],\n",
       "  ['lamb', 'NN', 'O'],\n",
       "  ['.', '.', 'O']],\n",
       " [['Peter', 'NNP', 'I-PER'], ['Blackburn', 'NNP', 'I-PER']],\n",
       " [['BRUSSELS', 'NNP', 'I-LOC'], ['1996-08-22', 'CD', 'O']],\n",
       " [['The', 'DT', 'O'],\n",
       "  ['European', 'NNP', 'I-ORG'],\n",
       "  ['Commission', 'NNP', 'I-ORG'],\n",
       "  ['said', 'VBD', 'O'],\n",
       "  ['on', 'IN', 'O'],\n",
       "  ['Thursday', 'NNP', 'O'],\n",
       "  ['it', 'PRP', 'O'],\n",
       "  ['disagreed', 'VBD', 'O'],\n",
       "  ['with', 'IN', 'O'],\n",
       "  ['German', 'JJ', 'I-MISC'],\n",
       "  ['advice', 'NN', 'O'],\n",
       "  ['to', 'TO', 'O'],\n",
       "  ['consumers', 'NNS', 'O'],\n",
       "  ['to', 'TO', 'O'],\n",
       "  ['shun', 'VB', 'O'],\n",
       "  ['British', 'JJ', 'I-MISC'],\n",
       "  ['lamb', 'NN', 'O'],\n",
       "  ['until', 'IN', 'O'],\n",
       "  ['scientists', 'NNS', 'O'],\n",
       "  ['determine', 'VBP', 'O'],\n",
       "  ['whether', 'IN', 'O'],\n",
       "  ['mad', 'JJ', 'O'],\n",
       "  ['cow', 'NN', 'O'],\n",
       "  ['disease', 'NN', 'O'],\n",
       "  ['can', 'MD', 'O'],\n",
       "  ['be', 'VB', 'O'],\n",
       "  ['transmitted', 'VBN', 'O'],\n",
       "  ['to', 'TO', 'O'],\n",
       "  ['sheep', 'NN', 'O'],\n",
       "  ['.', '.', 'O']],\n",
       " [['Germany', 'NNP', 'I-LOC'],\n",
       "  [\"'s\", 'POS', 'O'],\n",
       "  ['representative', 'NN', 'O'],\n",
       "  ['to', 'TO', 'O'],\n",
       "  ['the', 'DT', 'O'],\n",
       "  ['European', 'NNP', 'I-ORG'],\n",
       "  ['Union', 'NNP', 'I-ORG'],\n",
       "  [\"'s\", 'POS', 'O'],\n",
       "  ['veterinary', 'JJ', 'O'],\n",
       "  ['committee', 'NN', 'O'],\n",
       "  ['Werner', 'NNP', 'I-PER'],\n",
       "  ['Zwingmann', 'NNP', 'I-PER'],\n",
       "  ['said', 'VBD', 'O'],\n",
       "  ['on', 'IN', 'O'],\n",
       "  ['Wednesday', 'NNP', 'O'],\n",
       "  ['consumers', 'NNS', 'O'],\n",
       "  ['should', 'MD', 'O'],\n",
       "  ['buy', 'VB', 'O'],\n",
       "  ['sheepmeat', 'NN', 'O'],\n",
       "  ['from', 'IN', 'O'],\n",
       "  ['countries', 'NNS', 'O'],\n",
       "  ['other', 'JJ', 'O'],\n",
       "  ['than', 'IN', 'O'],\n",
       "  ['Britain', 'NNP', 'I-LOC'],\n",
       "  ['until', 'IN', 'O'],\n",
       "  ['the', 'DT', 'O'],\n",
       "  ['scientific', 'JJ', 'O'],\n",
       "  ['advice', 'NN', 'O'],\n",
       "  ['was', 'VBD', 'O'],\n",
       "  ['clearer', 'JJR', 'O'],\n",
       "  ['.', '.', 'O']]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dilution', 'Nano', '45.38', 'upgrade', 'Unless', 'lowest', 'wrap', 'sovereignty', 'Revised', 'erratically', '117.00', 'title', 'coffers', 'neighbouring', 'drug-related', '+91-11-3012024', 'partnership', 'Machines', 'Gong', 'correspondent']\n",
      "Vocab size: 23626\n",
      "['RBS', 'JJR', 'RBR', 'WP', ':', 'LS', 'FW', 'POS', 'JJ', 'CC', \"''\", '(', 'CD', 'WDT', 'VBD', '</s>', 'VBZ', 'UH', 'NN|SYM', 'WP$', '$', 'MD', 'RP', 'JJS', 'PRP', ')', 'NNP', 'VBG', 'NNS', '\"', '.', 'RB', 'EX', 'PDT', 'WRB', 'DT', ',', 'TO', 'NN', 'IN', 'VBN', 'SYM', 'VBP', 'VB', 'PRP$', '<s>', '<unk>', 'NNPS']\n",
      "['</s>', 'B-ORG', 'B-MISC', 'B-LOC', 'O', '<s>', '<unk>', 'I-PER', 'I-MISC', 'I-LOC', 'I-ORG']\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary - thank you w266\n",
    "# -- first attempt, leave in all numbers and maintain case\n",
    "flatData = [w for w in zip(*utils.flatten(trainSentences))]\n",
    "\n",
    "# try with lower vocab sizes... 10k, 15k, 20k\n",
    "vocab = vocabulary.Vocabulary( flatData[0])\n",
    "posTags = vocabulary.Vocabulary( flatData[1])\n",
    "nerTags = vocabulary.Vocabulary( flatData[2])\n",
    "\n",
    "print( (list(vocab.wordset)[:20]))\n",
    "print( \"Vocab size:\", vocab.size)\n",
    "print( (list(posTags.wordset)))\n",
    "print( (list(nerTags.wordset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This might be a good place to do some EDA on the vocab objects...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done playing around, define the data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conll2003Data(object):\n",
    "    \"\"\"\n",
    "    Keep track of data and processing operations for a single CoNLL2003 data file.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filePath):\n",
    "        \"\"\"\n",
    "        filePath(string): path to a CoNLL2003 raw data file for training the vocabulary\n",
    "        \"\"\"\n",
    "        # vocabulary objects for easy lookup\n",
    "        self.vocab = []\n",
    "        self.posTags = []\n",
    "        self.nerTags = []\n",
    "        \n",
    "        # read in training data\n",
    "        self.sentences = self.readFile(filePath)\n",
    "        \n",
    "    \n",
    "    def readFile( self, filePath):\n",
    "        \"\"\"\n",
    "        Read the conll2003 raw data file\n",
    "\n",
    "        filename(string) - path to conll2003 file (train, test, etc.)\n",
    "        \n",
    "        Returns: a list of lists of lists corresponding to the words, pos tags, and ner tags\n",
    "                 in each sentence\n",
    "\n",
    "        \"\"\"\n",
    "        f = open(filePath)\n",
    "        sentences = []\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            if len(line) == 0 or line.startswith(\"-DOCSTART\") or line[0] == '\\n':\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "            \n",
    "            # input format is [ word, pos tag, chunck tag, ner tag]\n",
    "            # we are ignoring the chunck tag\n",
    "            splits = line.strip().split(' ')\n",
    "            word = [splits[0], splits[1], splits[3]]\n",
    "            sentence.append( word)\n",
    "        \n",
    "        # don't forget the last one\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        \n",
    "        return sentences\n",
    "    \n",
    "    def buildVocab( self, vocabSize=None, verbose=False):\n",
    "        \"\"\"\n",
    "        Builds the vocabulary based on the initial data file\n",
    "        \n",
    "        vocabSize(int, default: None-all words) - max number of words to use for vocabulary\n",
    "                                                  (only used for training)\n",
    "        verbose(boolean, default: False)        - print extra info\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        flatData = [w for w in zip(*utils.flatten(self.sentences))]\n",
    "        self.vocab = vocabulary.Vocabulary( flatData[0], size=vocabSize)\n",
    "        \n",
    "        # remember these vocabs will have the <s>, </s>, and <unk> tags in there\n",
    "        # sizes need to be interpreted \"-3\" - consider replacing...\n",
    "        self.posTags = vocabulary.Vocabulary( flatData[1])\n",
    "        self.nerTags = vocabulary.Vocabulary( flatData[2])\n",
    "        \n",
    "        if verbose is True:\n",
    "            print( list(self.vocab.wordset)[:5], \"\\n\")\n",
    "            print( list(self.posTags.wordset)[:5], \"\\n\")\n",
    "            print( list(self.nerTags.wordset)[:5], \"\\n\")\n",
    "    \n",
    "    def formatPaddedData( self, sentences, clipPct = 5, verbose=False):\n",
    "        \"\"\"\n",
    "        Format the raw data by padding up to a max sentence length to be usable for training.\n",
    "        Make sure to call buildVocab first.\n",
    "        \n",
    "        sentences(list of lists of lists) - raw data from the CoNLL2003 dataset\n",
    "        clipPct(int, default: 5)          - the percentage of sentences to clip when determining the\n",
    "                                            max length to allow\n",
    "        verbose(boolean, default: False)  - print extra info\n",
    "        \n",
    "        Returns: 3 lists:   vocabulary converted to IDs, \n",
    "                            POS tags converted to IDs,\n",
    "                            NER label tag converted to IDs\n",
    "        \"\"\"\n",
    "        # get the maximum sentence length to clip down to (and pad up to) (add 2 for <s> and </s>)\n",
    "        self.maxSentLen = int(np.percentile( np.array([ len(s) for s in self.sentences]), 100.0 - clipPct)) + 2\n",
    "        \n",
    "        # we have a list of lists (sentences) of lists ([word, posTag, nerTag])\n",
    "        # parse through, pad each sentence with open and close tags, then convert to IDs\n",
    "        vocabIDs = [ self.vocab.words_to_ids( self.vocab.pad_sentence([ word[0] for word in sent])) \\\n",
    "                     for sent in sentences]\n",
    "        posIDs = [ self.posTags.words_to_ids( self.posTags.pad_sentence([word[1] for word in sent])) \\\n",
    "                   for sent in sentences]\n",
    "        nerIDs = [ self.nerTags.words_to_ids( ['<s>'] + [word[2] for word in sent] + ['</s>']) \\\n",
    "                   for sent in sentences]\n",
    "        \n",
    "        if verbose is True:\n",
    "            print( vocabIDs[:5], \"\\n\")\n",
    "            print( posIDs[:5], \"\\n\")\n",
    "            print( nerIDs[:5], \"\\n\")\n",
    "\n",
    "        # pad and clip so all sentences are the same length\n",
    "        vocabIDs = sequence.pad_sequences( vocabIDs, maxlen = self.maxSentLen)\n",
    "        posIDs = sequence.pad_sequences( posIDs, maxlen = self.maxSentLen)\n",
    "        nerIDs = sequence.pad_sequences( nerIDs, maxlen = self.maxSentLen, \n",
    "                                         value=self.nerTags.word_to_id['O'])\n",
    "        \n",
    "        return vocabIDs, posIDs, nerIDs\n",
    "    \n",
    "    #\n",
    "    # I think it makes the most sense to generate all the training data up front.\n",
    "    # If we had more data or planned to augment on the fly, it would make more sense to \n",
    "    # use a generator.\n",
    "    # \n",
    "    # window generation notes\n",
    "    #     Don't cross sentence boundaries.\n",
    "    #     This means that each sentence will be padded with (windowLength // 2) open/close\n",
    "    #     tags on each end. Also, when we hit a <s> tag after a </s> tag, start the new\n",
    "    #     window there rather than continuing to slide.\n",
    "    # \n",
    "    def formatWindowedData( self, sentences, windowLength=9, verbose=False):\n",
    "        \"\"\"\n",
    "        Format the raw data by blocking it into context windows of a fixed length corresponding \n",
    "        to the single target NER tag of the central word.\n",
    "        Make sure to call buildVocab first.\n",
    "        \n",
    "        sentences(list of lists of lists) - raw data from the CoNLL2003 dataset\n",
    "        windowLength(int, default: 9)     - The length of the context window\n",
    "                    NOTE - windowLength must be odd to have a central word. If itsn't, 1 will be added.\n",
    "        verbose(boolean, default: False)  - print extra info\n",
    "        \n",
    "        Returns: 3 numpy arrays: vocabulary training data windowed and converted to IDs, \n",
    "                                 POS tags windowed and converted to IDs,\n",
    "                                 NER label tags converted to IDs\n",
    "        \"\"\"\n",
    "        pads = windowLength // 2\n",
    "        \n",
    "        # we have a list of lists (sentences) of lists ([word, posTag, nerTag])\n",
    "        # parse through, pad each sentence with pads open and close tags, then convert to IDs\n",
    "        vocabIDs = [ self.vocab.words_to_ids( [\"<s>\"] * pads + [word[0] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                     for sent in sentences]\n",
    "        posIDs = [ self.posTags.words_to_ids( [\"<s>\"] * pads + [word[1] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                   for sent in sentences]\n",
    "        nerIDs = [ self.nerTags.words_to_ids( [\"<s>\"] * pads + [word[2] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                   for sent in sentences]\n",
    "        \n",
    "        if verbose is True:\n",
    "            print( vocabIDs[:5], \"\\n\")\n",
    "            print( posIDs[:5], \"\\n\")\n",
    "            print( nerIDs[:5], \"\\n\")\n",
    "        \n",
    "        assert(len(vocabIDs) == len(posIDs) and len(posIDs) == len(nerIDs))\n",
    "        \n",
    "        # build the data to train on by sliding the window across each sentence\n",
    "        # at this point, all 3 lists are the same size, so we can run through them all at once\n",
    "        featsVocab, featsPOS, featsNER = [], [], []\n",
    "        for sentID in range( len(vocabIDs)):\n",
    "            sent = vocabIDs[sentID]\n",
    "            sentPOS = posIDs[sentID]\n",
    "            sentNER = nerIDs[sentID]\n",
    "            \n",
    "            for ID in range( len(sent) - windowLength + 1):\n",
    "                featsVocab.append( sent[ID:ID + windowLength])\n",
    "                featsPOS.append( sentPOS[ID:ID + windowLength])\n",
    "                featsNER.append( sentNER[ID + windowLength // 2])\n",
    "        \n",
    "        return np.array(featsVocab), \\\n",
    "               np.array(featsPOS), \\\n",
    "               np.array(featsNER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the windowing functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = trainSentences\n",
    "windowLength = 9\n",
    "pads = windowLength // 2\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 959, 11985, 235, 764, 8, 4149, 211, 6184, 3, 1, 1, 1, 1], [0, 0, 0, 0, 734, 2070, 1, 1, 1, 1], [0, 0, 0, 0, 1381, 136, 1, 1, 1, 1]] \n",
      "\n",
      "[[0, 0, 0, 0, 3, 22, 8, 4, 17, 13, 8, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 5, 1, 1, 1, 1]] \n",
      "\n",
      "[[0, 0, 0, 0, 5, 3, 7, 3, 3, 3, 7, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 4, 4, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 1, 1, 1, 1]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we have a list of lists (sentences) of lists ([word, posTag, nerTag])\n",
    "# parse through, pad each sentence with open and close tags, then convert to IDs\n",
    "vocabIDs = [ vocab.words_to_ids( [\"<s>\"] * pads + [word[0] for word in sent] + [\"</s>\"] * pads) \\\n",
    "             for sent in sentences]\n",
    "posIDs = [ posTags.words_to_ids( [\"<s>\"] * pads + [word[1] for word in sent] + [\"</s>\"] * pads) \\\n",
    "           for sent in sentences]\n",
    "nerIDs = [ nerTags.words_to_ids( [\"<s>\"] * pads + [word[2] for word in sent] + [\"</s>\"] * pads) \\\n",
    "           for sent in sentences]\n",
    "\n",
    "if verbose is True:\n",
    "    print( vocabIDs[:3], \"\\n\")\n",
    "    print( posIDs[:3], \"\\n\")\n",
    "    print( nerIDs[:3], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 959, 11985, 235, 764, 8, 4149, 211, 6184, 3, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 3, 22, 8, 4, 17, 13, 8, 4, 11, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 5, 3, 7, 3, 3, 3, 7, 3, 3, 1, 1, 1, 1] \n",
      "\n",
      "[0, 0, 0, 0, 734, 2070, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 3, 3, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 4, 4, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "[0, 0, 0, 0, 959, 11985, 235, 764, 8]\n",
      "[0, 0, 0, 0, 3, 22, 8, 4, 17]\n",
      "Center NER tag: 5\n",
      "Center ID: 959 \n",
      "\n",
      "[0, 0, 0, 959, 11985, 235, 764, 8, 4149]\n",
      "[0, 0, 0, 3, 22, 8, 4, 17, 13]\n",
      "Center NER tag: 3\n",
      "Center ID: 11985 \n",
      "\n",
      "[0, 0, 959, 11985, 235, 764, 8, 4149, 211]\n",
      "[0, 0, 3, 22, 8, 4, 17, 13, 8]\n",
      "Center NER tag: 7\n",
      "Center ID: 235 \n",
      "\n",
      "[0, 959, 11985, 235, 764, 8, 4149, 211, 6184]\n",
      "[0, 3, 22, 8, 4, 17, 13, 8, 4]\n",
      "Center NER tag: 3\n",
      "Center ID: 764 \n",
      "\n",
      "[959, 11985, 235, 764, 8, 4149, 211, 6184, 3]\n",
      "[3, 22, 8, 4, 17, 13, 8, 4, 11]\n",
      "Center NER tag: 3\n",
      "Center ID: 8 \n",
      "\n",
      "[11985, 235, 764, 8, 4149, 211, 6184, 3, 1]\n",
      "[22, 8, 4, 17, 13, 8, 4, 11, 1]\n",
      "Center NER tag: 3\n",
      "Center ID: 4149 \n",
      "\n",
      "[235, 764, 8, 4149, 211, 6184, 3, 1, 1]\n",
      "[8, 4, 17, 13, 8, 4, 11, 1, 1]\n",
      "Center NER tag: 7\n",
      "Center ID: 211 \n",
      "\n",
      "[764, 8, 4149, 211, 6184, 3, 1, 1, 1]\n",
      "[4, 17, 13, 8, 4, 11, 1, 1, 1]\n",
      "Center NER tag: 3\n",
      "Center ID: 6184 \n",
      "\n",
      "[8, 4149, 211, 6184, 3, 1, 1, 1, 1]\n",
      "[17, 13, 8, 4, 11, 1, 1, 1, 1]\n",
      "Center NER tag: 3\n",
      "Center ID: 3 \n",
      "\n",
      "[0, 0, 0, 0, 734, 2070, 1, 1, 1]\n",
      "[0, 0, 0, 0, 3, 3, 1, 1, 1]\n",
      "Center NER tag: 4\n",
      "Center ID: 734 \n",
      "\n",
      "[0, 0, 0, 734, 2070, 1, 1, 1, 1]\n",
      "[0, 0, 0, 3, 3, 1, 1, 1, 1]\n",
      "Center NER tag: 4\n",
      "Center ID: 2070 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build the data to train on by sliding the window across each sentence\n",
    "# at this point, all 3 lists are the same size, so we can run through them all at once\n",
    "featsVocab, featsPOS, featsNER = [], [], []\n",
    "for sentID in range( len(vocabIDs)):\n",
    "    sent = vocabIDs[sentID]\n",
    "    sentPOS = posIDs[sentID]\n",
    "    sentNER = nerIDs[sentID]\n",
    "    for ID in range( len(sent) - windowLength + 1):\n",
    "        featsVocab.append( sent[ID:ID + windowLength])\n",
    "        featsPOS.append( sentPOS[ID:ID + windowLength])\n",
    "        featsNER.append( sentNER[ID + windowLength // 2])\n",
    "\n",
    "showID = 0\n",
    "print( vocabIDs[showID])\n",
    "print( posIDs[showID])\n",
    "print( nerIDs[showID], '\\n')\n",
    "print( vocabIDs[showID+1])\n",
    "print( posIDs[showID+1])\n",
    "print( nerIDs[showID+1])\n",
    "print( '\\n')\n",
    "for i in range(11):\n",
    "    print( featsVocab[i])\n",
    "    print( featsPOS[i])\n",
    "    print( \"Center NER tag:\", featsNER[i])\n",
    "    print( \"Center ID:\", featsVocab[i][windowLength // 2], '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 959, 11985, 235, 764, 8, 4149, 211, 6184, 3, 1, 1, 1, 1], [0, 0, 0, 0, 734, 2070, 1, 1, 1, 1], [0, 0, 0, 0, 1381, 136, 1, 1, 1, 1], [0, 0, 0, 0, 20, 228, 457, 15, 14, 68, 37, 8129, 26, 235, 4150, 8, 2478, 8, 11986, 211, 6184, 409, 3544, 2071, 501, 1791, 1922, 653, 289, 41, 8130, 8, 1923, 3, 1, 1, 1, 1], [0, 0, 0, 0, 116, 16, 3112, 8, 5, 228, 487, 16, 2752, 1060, 8131, 8132, 15, 14, 73, 2478, 259, 876, 8133, 28, 539, 126, 114, 124, 409, 5, 2479, 4150, 21, 11987, 3, 1, 1, 1, 1]] \n",
      "\n",
      "[[0, 0, 0, 0, 3, 22, 8, 4, 17, 13, 8, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 5, 1, 1, 1, 1], [0, 0, 0, 0, 7, 3, 3, 10, 6, 3, 18, 10, 6, 8, 4, 17, 9, 17, 13, 8, 4, 6, 9, 27, 6, 8, 4, 4, 28, 13, 14, 17, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 25, 4, 17, 7, 3, 3, 25, 8, 4, 3, 3, 10, 6, 3, 9, 28, 13, 4, 6, 9, 8, 6, 3, 6, 7, 8, 4, 10, 36, 11, 1, 1, 1, 1]] \n",
      "\n",
      "[[0, 0, 0, 0, 5, 3, 7, 3, 3, 3, 7, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 4, 4, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 5, 5, 3, 3, 3, 3, 3, 3, 7, 3, 3, 3, 3, 3, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 3, 3, 3, 5, 5, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1]] \n",
      "\n",
      "Windowed sentence length: 9\n",
      "[[    0     0     0     0   959 11985   235   764     8]\n",
      " [    0     0     0   959 11985   235   764     8  4149]\n",
      " [    0     0   959 11985   235   764     8  4149   211]\n",
      " [    0   959 11985   235   764     8  4149   211  6184]\n",
      " [  959 11985   235   764     8  4149   211  6184     3]\n",
      " [11985   235   764     8  4149   211  6184     3     1]\n",
      " [  235   764     8  4149   211  6184     3     1     1]\n",
      " [  764     8  4149   211  6184     3     1     1     1]\n",
      " [    8  4149   211  6184     3     1     1     1     1]\n",
      " [    0     0     0     0   734  2070     1     1     1]] \n",
      " [5 3 7 3 3 3 7 3 3 4]\n"
     ]
    }
   ],
   "source": [
    "#clipPct = 5\n",
    "windowLength = 9\n",
    "testNumSents = 5000\n",
    "\n",
    "# read in training data\n",
    "vocabData = conll2003Data( TRAIN_FILE)\n",
    "\n",
    "# FOR TESTING\n",
    "#vocabData.sentences = vocabData.sentences[:testNumSents]\n",
    "\n",
    "# not yet using the pos tags\n",
    "# try with lower vocab sizes... 10k, 15k, 20k\n",
    "#vocabData.buildVocab( vocabSize=20000)\n",
    "vocabData.buildVocab( vocabSize=20000)\n",
    "trainX, _, trainY = vocabData.formatWindowedData( vocabData.sentences, \n",
    "                                                  windowLength=windowLength,\n",
    "                                                  verbose=True)\n",
    "\n",
    "# check the first few windows\n",
    "print( \"Windowed sentence length:\", len(trainX[0]))\n",
    "print( trainX[:10], '\\n', trainY[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 452, 19, 2, 9044, 8349, 128, 7588, 1828, 16059, 10368, 3, 1, 1, 1, 1], [0, 0, 0, 0, 212, 2749, 1, 1, 1, 1], [0, 0, 0, 0, 292, 905, 6372, 3615, 6347, 266, 155, 17, 1373, 14, 93, 32, 2530, 58, 3173, 29, 42, 274, 12, 1732, 371, 7, 54, 281, 8, 255, 72, 25, 5, 549, 6, 5, 2191, 467, 3, 1, 1, 1, 1], [0, 0, 0, 0, 2501, 1209, 14, 426, 4, 1767, 4, 496, 41, 17973, 32, 859, 2761, 1500, 4, 7170, 12, 1820, 129, 519, 7, 14, 302, 215, 2815, 183, 61, 17, 188, 115, 7, 56, 19891, 149, 77, 2531, 3, 1, 1, 1, 1], [0, 0, 0, 0, 932, 5882, 3173, 67, 17, 3338, 14, 5, 739, 551, 25, 2, 5879, 4, 2530, 3589, 56, 49, 274, 29, 6350, 371, 111, 234, 4479, 67, 17, 2, 26, 134, 2, 2841, 10895, 450, 87, 17, 3338, 3, 1, 1, 1, 1]] \n",
      "\n",
      "[[0, 0, 0, 0, 3, 23, 3, 3, 6, 3, 3, 3, 3, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 5, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 4, 3, 3, 10, 5, 6, 5, 6, 3, 6, 3, 10, 3, 6, 7, 4, 16, 5, 9, 6, 5, 9, 17, 13, 6, 6, 7, 4, 6, 7, 4, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 26, 4, 6, 4, 12, 15, 12, 28, 13, 8, 6, 4, 9, 3, 12, 3, 16, 3, 7, 10, 31, 6, 4, 6, 3, 10, 31, 6, 14, 4, 6, 26, 8, 4, 6, 3, 11, 1, 1, 1, 1], [0, 0, 0, 0, 6, 21, 3, 31, 6, 5, 6, 7, 4, 4, 6, 3, 3, 12, 3, 10, 26, 8, 4, 6, 5, 22, 6, 21, 10, 31, 6, 5, 6, 3, 27, 3, 3, 21, 5, 6, 5, 11, 1, 1, 1, 1]] \n",
      "\n",
      "[[0, 0, 0, 0, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 1, 1, 1, 1], [0, 0, 0, 0, 7, 7, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 5, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3, 5, 3, 5, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 6, 6, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 4, 4, 3, 3, 3, 3, 3, 1, 1, 1, 1]] \n",
      "\n",
      "Padded sentence length: 9\n",
      "[[    0     0     0     0   452    19     2  9044  8349]\n",
      " [    0     0     0   452    19     2  9044  8349   128]\n",
      " [    0     0   452    19     2  9044  8349   128  7588]\n",
      " [    0   452    19     2  9044  8349   128  7588  1828]\n",
      " [  452    19     2  9044  8349   128  7588  1828 16059]\n",
      " [   19     2  9044  8349   128  7588  1828 16059 10368]\n",
      " [    2  9044  8349   128  7588  1828 16059 10368     3]\n",
      " [ 9044  8349   128  7588  1828 16059 10368     3     1]\n",
      " [ 8349   128  7588  1828 16059 10368     3     1     1]\n",
      " [  128  7588  1828 16059 10368     3     1     1     1]\n",
      " [ 7588  1828 16059 10368     3     1     1     1     1]\n",
      " [    0     0     0     0   212  2749     1     1     1]] \n",
      " [3 3 5 3 3 3 3 3 3 3 3 6]\n"
     ]
    }
   ],
   "source": [
    "# read in dev data\n",
    "devSents = vocabData.readFile( DEV_FILE)\n",
    "\n",
    "# FOR TESTING\n",
    "#devSents = devSents[:testNumSents]\n",
    "\n",
    "# not yet using the pos tags\n",
    "devX, _, devY = vocabData.formatWindowedData( devSents, \n",
    "                                              windowLength=windowLength,\n",
    "                                              verbose=True)\n",
    "\n",
    "print( \"Padded sentence length:\", len(devX[0]))\n",
    "print( devX[:12], '\\n', devY[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 127, 19, 2, 2, 2, 1436, 4, 2, 471, 2, 15306, 3, 1, 1, 1, 1], [0, 0, 0, 0, 2, 2, 1, 1, 1, 1], [0, 0, 0, 0, 2, 4, 167, 1124, 12138, 2, 1, 1, 1, 1], [0, 0, 0, 0, 216, 531, 5, 1136, 6, 56, 5300, 206, 859, 26, 9, 9010, 934, 181, 77, 1674, 7, 9, 957, 1772, 467, 149, 14, 93, 3, 1, 1, 1, 1], [0, 0, 0, 0, 108, 202, 1269, 56, 6568, 13066, 186, 7, 5, 81, 149, 6, 5, 139, 4, 6959, 8, 9, 3245, 750, 1088, 8, 2, 2, 3, 1, 1, 1, 1]] \n",
      "\n",
      "[[0, 0, 0, 0, 4, 23, 3, 13, 3, 3, 12, 3, 6, 7, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 12, 3, 3, 29, 5, 1, 1, 1, 1], [0, 0, 0, 0, 3, 10, 7, 4, 6, 26, 8, 3, 4, 6, 7, 8, 5, 27, 6, 3, 6, 7, 3, 3, 4, 4, 6, 3, 11, 1, 1, 1, 1], [0, 0, 0, 0, 16, 3, 10, 26, 4, 13, 18, 6, 7, 4, 4, 6, 7, 4, 12, 21, 17, 7, 4, 5, 4, 17, 9, 3, 11, 1, 1, 1, 1]] \n",
      "\n",
      "[[0, 0, 0, 0, 3, 3, 6, 3, 3, 3, 3, 4, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 4, 4, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 6, 6, 6, 3, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 3, 3, 3, 3, 7, 7, 3, 3, 3, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 1, 1, 1, 1]] \n",
      "\n",
      "Padded sentence length: 9\n",
      "[[    0     0     0     0   127    19     2     2     2]\n",
      " [    0     0     0   127    19     2     2     2  1436]\n",
      " [    0     0   127    19     2     2     2  1436     4]\n",
      " [    0   127    19     2     2     2  1436     4     2]\n",
      " [  127    19     2     2     2  1436     4     2   471]\n",
      " [   19     2     2     2  1436     4     2   471     2]\n",
      " [    2     2     2  1436     4     2   471     2 15306]\n",
      " [    2     2  1436     4     2   471     2 15306     3]\n",
      " [    2  1436     4     2   471     2 15306     3     1]\n",
      " [ 1436     4     2   471     2 15306     3     1     1]\n",
      " [    4     2   471     2 15306     3     1     1     1]\n",
      " [    2   471     2 15306     3     1     1     1     1]] \n",
      " [3 3 6 3 3 3 3 4 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "# read in the test data\n",
    "testSents = vocabData.readFile( TEST_FILE)\n",
    "\n",
    "# not yet using the pos tags\n",
    "testX, _, testY = vocabData.formatWindowedData( testSents, \n",
    "                                                windowLength=windowLength,\n",
    "                                                verbose=True)\n",
    "\n",
    "print( \"Padded sentence length:\", len(testX[0]))\n",
    "print( testX[:12], '\\n', testY[:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -  \n",
    "### Investigate NER tags\n",
    "> We can see that the dev set doesn't have `B-LOG` or `B-ORG`.  \n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'I-PER', 'I-ORG', 'I-LOC', 'I-MISC', 'B-MISC', 'B-ORG', 'B-LOC']\n",
      "{'</s>', 'B-ORG', 'B-MISC', 'B-LOC', 'O', '<s>', '<unk>', 'I-PER', 'I-MISC', 'I-LOC', 'I-ORG'}\n"
     ]
    }
   ],
   "source": [
    "s = set([i for i in trainY.flatten()])\n",
    "print(vocabData.nerTags.ids_to_words(s))\n",
    "print(vocabData.nerTags.wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'I-PER', 'I-ORG', 'I-LOC', 'I-MISC', 'B-MISC']\n",
      "{'</s>', 'B-ORG', 'B-MISC', 'B-LOC', 'O', '<s>', '<unk>', 'I-PER', 'I-MISC', 'I-LOC', 'I-ORG'}\n"
     ]
    }
   ],
   "source": [
    "s = set([i for i in devY.flatten()])\n",
    "print(vocabData.nerTags.ids_to_words(s))\n",
    "print(vocabData.nerTags.wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'I-PER', 'I-ORG', 'I-LOC', 'I-MISC', 'B-MISC', 'B-ORG', 'B-LOC']\n",
      "{'</s>', 'B-ORG', 'B-MISC', 'B-LOC', 'O', '<s>', '<unk>', 'I-PER', 'I-MISC', 'I-LOC', 'I-ORG'}\n"
     ]
    }
   ],
   "source": [
    "s = set([i for i in testY.flatten()])\n",
    "print(vocabData.nerTags.ids_to_words(s))\n",
    "print(vocabData.nerTags.wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 169578, 'I-PER': 11128, 'I-ORG': 10001, 'I-LOC': 8286, 'I-MISC': 4556, 'B-MISC': 37, 'B-ORG': 24, 'B-LOC': 11})\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print( vocabData.nerTags.unigram_counts)\n",
    "print( len(vocabData.nerTags.unigram_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 O \t 169578\n",
      "4 I-PER \t 11128\n",
      "5 I-ORG \t 10001\n",
      "6 I-LOC \t 8286\n",
      "7 I-MISC \t 4556\n",
      "8 B-MISC \t 37\n",
      "9 B-ORG \t 24\n",
      "10 B-LOC \t 11\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "bc = Counter()\n",
    "a = []\n",
    "flatX = trainX.flatten()\n",
    "flatY = trainY.flatten()\n",
    "for j, i in enumerate(flatY):\n",
    "    if i == 10:\n",
    "        a += [j]\n",
    "        #print(vocabData.vocab.id_to_word[flatX[j]])\n",
    "    bc[int(i)] += 1\n",
    "\n",
    "for key, v in sorted(bc.items(), key=lambda i: i[0]):\n",
    "    print( key, vocabData.nerTags.id_to_word[int(key)], '\\t', v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1832589\n",
      "203621\n"
     ]
    }
   ],
   "source": [
    "print(len(flatX))\n",
    "print(len(flatY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1819, 126796, 126806, 126815, 126825, 126834, 126842, 126852, 126861, 126870, 160096]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-510ea7225652>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mvocabData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mvocabData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnerTags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflatY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnerTags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "for j in a:\n",
    "    print( vocabData.vocab.id_to_word[trainX[j]], vocabData.vocab.ids_to_words(flatX[j-4:j+4]))\n",
    "    print( vocabData.nerTags.id_to_word[flatY[j]], vocabData.nerTags.ids_to_words(flatY[j-4:j+4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capsule layers from Xifeng Guo \n",
    "# https://github.com/XifengGuo/CapsNet-Keras\n",
    "from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape y from all labels to a row per class\n",
    "trainY_cat = to_categorical(trainY.astype('float32'))\n",
    "# num_classes is required because the dev set doesn't have all the tags represented\n",
    "devY_cat = to_categorical(devY.astype('float32'), num_classes=trainY_cat.shape[1])\n",
    "testY_cat = to_categorical(testY.astype('float32'), num_classes=trainY_cat.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainY_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 0\n",
      "2 0\n",
      "3 169578\n",
      "4 180706\n",
      "5 190707\n",
      "6 198993\n",
      "7 203549\n",
      "8 203586\n",
      "9 203610\n",
      "10 203621\n",
      "New shape of train: (203621, 11)\n",
      "New shape of dev: (51362, 11)\n",
      "New shape of test: (46435, 11)\n"
     ]
    }
   ],
   "source": [
    "# look at the 1-hot representation\n",
    "totes = 0\n",
    "for n in range(vocabData.nerTags.size):\n",
    "    for i in trainY_cat:\n",
    "        if i[n] == 1:\n",
    "            totes += 1\n",
    "    print(n, totes)\n",
    "\n",
    "print( \"New shape of train:\", trainY_cat.shape)\n",
    "print( \"New shape of dev:\", devY_cat.shape)\n",
    "print( \"New shape of test:\", testY_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have a few extra tags in the NER vocab for padding and such. We'll shrink the vectors and remove these unnecessary targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), trainY_cat)), dtype=np.float)\n",
    "devY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), devY_cat)), dtype=np.float)\n",
    "testY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), testY_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 169578\n",
      "1 180706\n",
      "2 190707\n",
      "3 198993\n",
      "4 203549\n",
      "5 203586\n",
      "6 203610\n",
      "7 203621\n",
      "New shape of train: (203621, 8)\n",
      "New shape of dev: (51362, 8)\n"
     ]
    }
   ],
   "source": [
    "# Make sure it worked\n",
    "totes = 0\n",
    "for n in range(trainY_cat.shape[1]):\n",
    "    for i in trainY_cat:\n",
    "        if i[n] == 1:\n",
    "            totes += 1\n",
    "    print(n, totes)\n",
    "\n",
    "print( \"New shape of train:\", trainY_cat.shape)\n",
    "print( \"New shape of dev:\", devY_cat.shape)\n",
    "#print( \"New shape of test:\", testY_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabData.vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-2bbf4bc2a50a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load train and test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "# Load train and test data\n",
    "max_features = vocabData.vocab.size\n",
    "x_train, y_train, x_test, y_test = tf.keras.datasets.reuters.load_data(num_words=max_features)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparameters\n",
    "max_features = vocabData.vocab.size\n",
    "maxlen = trainX.shape[1]\n",
    "ner_classes = trainY_cat.shape[1]\n",
    "embed_dim = 50\n",
    "num_routing = 3\n",
    "\n",
    "save_dir = './result'\n",
    "batch_size = 100\n",
    "debug = 2\n",
    "epochs = 10\n",
    "dropout_p = 0.25\n",
    "embed_dropout = 0.25\n",
    "lam_recon = 0.0005\n",
    "\n",
    "#Load train and test data\n",
    "#(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_features)\n",
    "#x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "#x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-2bbf4bc2a50a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load train and test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "# Load train and test data\n",
    "max_features = vocabData.vocab.size\n",
    "x_train, y_train, x_test, y_test = tf.keras.datasets.reuters.load_data(num_words=max_features)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed (?, 9, 50)\n",
      "WARNING:tensorflow:From /home/ith/Practice/capsnet_NLP/capsulelayers.py:137: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "ner_caps (?, 8, 16)\n",
      "out_caps (?, 8)\n"
     ]
    }
   ],
   "source": [
    "#from keras import Sequential\n",
    "#Build embedding and convolutional layers\n",
    "\n",
    "x = Input(shape=(maxlen,))\n",
    "embed = Embedding(max_features, embed_dim, input_length=maxlen, embeddings_initializer=\"random_uniform\" )(x)\n",
    "\n",
    "# only needed for conv2D (ie without conv1D...)\n",
    "#embed = Reshape( ( maxlen, embed_dim, 1))(embed)\n",
    "\n",
    "#add dropout here... spatial1d I think it is...\n",
    "#embed = SpatialDropout1D(embed_dropout)(embed)\n",
    "\n",
    "# maybe add this back in!! - make sure to consider the window length\n",
    "#conv1 = Conv1D( filters=256, kernel_size=3, strides=1, padding='valid', \n",
    "#                      activation='relu', name='conv1')(embed)\n",
    "\n",
    "print( \"embed\", embed.get_shape())\n",
    "    \n",
    "# Layer 2: Conv2D layer with `squash` activation, then reshape to \n",
    "# kernel_size should be smaller than window size... maybe half of window size?\n",
    "# [None, num_capsule, dim_vector]\n",
    "primarycaps = PrimaryCap( embed, dim_capsule=8, \n",
    "                          n_channels=32, kernel_size=(windowLength // 2),#, embed_dim),#<-- make it like 1D windowLength // 2, \n",
    "                          strides=1, padding='valid')\n",
    "\n",
    "# Layer 3: Capsule layer. Routing algorithm works here.\n",
    "ner_caps = CapsuleLayer( num_capsule=ner_classes, dim_capsule=16, \n",
    "                         routings=num_routing, name='nercaps')(primarycaps)\n",
    "\n",
    "\n",
    "# Layer 4: This is an auxiliary layer to replace each capsule with its length. \n",
    "# Just to match the true label's shape.\n",
    "# If using tensorflow, this will not be necessary. :)\n",
    "print( \"ner_caps\", ner_caps.get_shape())\n",
    "out_caps = Length(name='out_caps')(ner_caps)\n",
    "print( \"out_caps\", out_caps.get_shape())\n",
    "\n",
    "# Decoder network. - probably get the dims right then flatten it\n",
    "\n",
    "\n",
    "#not these...\n",
    "#y = Reshape( ( maxlen * ner_classes,), input_shape=( trainY.shape[1], trainY.shape[2],))\n",
    "#y = Flatten( )(y)\n",
    "#y = KB.reshape( y, ( maxlen * ner_classes,))\n",
    "#print(\"y shape:\", y.)\n",
    "#print(ner_caps.shape[1], ner_caps.shape[2], ner_caps.shape[1] * ner_caps.shape[2])\n",
    "#ner_caps = Reshape((ner_caps.shape[1] * ner_caps.shape[2],))(ner_caps)\n",
    "\n",
    "\n",
    "#y = Input(shape=(ner_classes,))\n",
    "\n",
    "#masked = Mask()([ner_caps, y])  # The true label is used to mask the output of capsule layer.\n",
    "#x_recon = Dense(512, activation='relu')(y)#(masked)\n",
    "#x_recon = Dense(1024, activation='relu')(x_recon)\n",
    "#x_recon = Dense(maxlen, activation='sigmoid')(x_recon)\n",
    "\n",
    "# maybe change to a sampled softmax!\n",
    "#x_recon = Dense( max_features, activation='softmax')(x_recon)\n",
    "\n",
    "\n",
    "# x_recon = layers.Reshape(target_shape=[1], name='out_recon')(x_recon)\n",
    "\n",
    "capsmodel = Model([x], [out_caps])\n",
    "#capsmodel = Model([x, y], [out_caps, x_recon])\n",
    "#capsmodel = Model([y], [x_recon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot==1.2.3 in /home/ith/anaconda3/lib/python3.6/site-packages (1.2.3)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/ith/anaconda3/lib/python3.6/site-packages (from pydot==1.2.3) (2.2.0)\n",
      "Requirement already satisfied: pydot_ng in /home/ith/anaconda3/lib/python3.6/site-packages (1.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in /home/ith/anaconda3/lib/python3.6/site-packages (from pydot_ng) (2.2.0)\n",
      "Requirement already satisfied: pydotplus in /home/ith/anaconda3/lib/python3.6/site-packages (2.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in /home/ith/anaconda3/lib/python3.6/site-packages (from pydotplus) (2.2.0)\n",
      "Requirement already satisfied: graphviz in /home/ith/anaconda3/lib/python3.6/site-packages (0.8.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot==1.2.3\n",
    "!pip install pydot_ng\n",
    "!pip install pydotplus\n",
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             1000000   \n",
      "_________________________________________________________________\n",
      "primarycap_conv2d (Conv1D)   (None, 6, 256)            51456     \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 192, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 192, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             196608    \n",
      "_________________________________________________________________\n",
      "out_caps (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,248,064\n",
      "Trainable params: 1,248,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "{'dot': '/usr/bin/dot', 'twopi': '/usr/bin/twopi', 'neato': '/usr/bin/neato', 'circo': '/usr/bin/circo', 'fdp': '/usr/bin/fdp', 'sfdp': '/usr/bin/sfdp'}\n"
     ]
    }
   ],
   "source": [
    "#Saving weights and logging\n",
    "log = callbacks.CSVLogger(save_dir + '/log.csv')\n",
    "tb = callbacks.TensorBoard(log_dir=save_dir + '/tensorboard-logs', \n",
    "                           batch_size=batch_size, histogram_freq=debug)\n",
    "checkpoint = callbacks.ModelCheckpoint(save_dir + '/weights-{epoch:02d}.h5', \n",
    "                                       save_best_only=True, \n",
    "                                       save_weights_only=True, \n",
    "                                       verbose=1)\n",
    "lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
    "\n",
    "#margin_loss\n",
    "def margin_loss(y_true, y_pred):\n",
    "    L = y_true * KB.square(KB.maximum(0., 0.9 - y_pred)) + 0.5 * (1 - y_true) * KB.square(KB.maximum(0., y_pred - 0.1))\n",
    "    return KB.mean(KB.sum(L, 1))\n",
    "\n",
    "capsmodel.summary()\n",
    "\n",
    "#Save a png of the model shapes and flow\n",
    "import pydot_ng as pydot\n",
    "print (pydot.find_graphviz())\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(capsmodel, to_file='capsmodel.png', show_shapes=True, show_layer_names=True)\n",
    "#plot_model(capsmodel, to_file=save_dir + '/reuters-model.png', show_shapes=True)\n",
    "\n",
    "opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#opt = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/10\n",
      "203621/203621 [==============================] - 455s 2ms/step - loss: 0.1269 - acc: 0.8778 - val_loss: 0.0296 - val_acc: 0.9619\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02956, saving model to ./result/weights-01.h5\n",
      "Epoch 2/10\n",
      "203621/203621 [==============================] - 566s 3ms/step - loss: 0.0097 - acc: 0.9887 - val_loss: 0.0262 - val_acc: 0.9666\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02956 to 0.02617, saving model to ./result/weights-02.h5\n",
      "Epoch 3/10\n",
      "203621/203621 [==============================] - 609s 3ms/step - loss: 0.0040 - acc: 0.9954 - val_loss: 0.0259 - val_acc: 0.9681\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02617 to 0.02593, saving model to ./result/weights-03.h5\n",
      "Epoch 4/10\n",
      "203621/203621 [==============================] - 585s 3ms/step - loss: 0.0024 - acc: 0.9974 - val_loss: 0.0287 - val_acc: 0.9643\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.02593\n",
      "Epoch 5/10\n",
      "203621/203621 [==============================] - 1018s 5ms/step - loss: 0.0017 - acc: 0.9982 - val_loss: 0.0293 - val_acc: 0.9660\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.02593\n",
      "Epoch 6/10\n",
      "203621/203621 [==============================] - 727s 4ms/step - loss: 0.0013 - acc: 0.9986 - val_loss: 0.0286 - val_acc: 0.9659\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02593\n",
      "Epoch 7/10\n",
      "203621/203621 [==============================] - 567s 3ms/step - loss: 0.0011 - acc: 0.9988 - val_loss: 0.0316 - val_acc: 0.9644\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02593\n",
      "Epoch 8/10\n",
      "203621/203621 [==============================] - 562s 3ms/step - loss: 9.6401e-04 - acc: 0.9989 - val_loss: 0.0302 - val_acc: 0.9652\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02593\n",
      "Epoch 9/10\n",
      "203621/203621 [==============================] - 537s 3ms/step - loss: 9.0885e-04 - acc: 0.9989 - val_loss: 0.0319 - val_acc: 0.9638\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02593\n",
      "Epoch 10/10\n",
      "203621/203621 [==============================] - 744s 4ms/step - loss: 8.1537e-04 - acc: 0.9991 - val_loss: 0.0351 - val_acc: 0.9612\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02593\n"
     ]
    }
   ],
   "source": [
    "# train the model - no recon\n",
    "# compile the model\n",
    "capsmodel.compile(optimizer=opt, #'adam',\n",
    "              loss=margin_loss,\n",
    "              metrics={'out_caps': 'accuracy'})\n",
    "\n",
    "data = capsmodel.fit( trainX, \n",
    "                      trainY_cat, \n",
    "                      batch_size=batch_size, \n",
    "                      epochs=epochs, \n",
    "                      validation_data=[devX, devY_cat], \n",
    "                      callbacks=[log, tb, checkpoint], \n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history( history):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-3c162699a834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     }\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-3c162699a834>\u001b[0m in \u001b[0;36mscore_eval\u001b[0;34m(capsmodel, testX, testY)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscore_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapsmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcapsmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     return {\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'History' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, confusion_matrix, classification_report\n",
    "from keras.callbacks import Callback\n",
    "def score_eval(capsmodel, testX, testY):\n",
    "    preds = capsmodel.predict(testX)\n",
    "    return {\n",
    "        'roc_auc': roc_auc_score(testY, preds),\n",
    "        'accuracy': accuracy_score(testY, preds),\n",
    "        'confmat': confusion_matrix(testY, preds),\n",
    "        'clf_rep': classification_report(testY, preds)\n",
    "    }\n",
    "\n",
    "results = score_eval(data, testX, testY)\n",
    "\n",
    "print('-'*60)\n",
    "for key, value in results.items():\n",
    "    print(key)\n",
    "    print(value)\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5268534160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5268446e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------Begin: test------------------------------\n",
      "Test acc: 0.9369225799504683\n"
     ]
    }
   ],
   "source": [
    "plot_history(data)\n",
    "y_pred = capsmodel.predict(testX, batch_size=100)\n",
    "print('-'*30 + 'Begin: test' + '-'*30)\n",
    "print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(testY_cat, 1))/testY_cat.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21320072 0.35533453 0.21320072 0.28426762 0.49746834 0.21320072\n",
      " 0.63960215]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "testPred = [ 3, 5, 3, 4, 7, 3, 9]\n",
    "testPred = np.array(testPred / np.linalg.norm(testPred))\n",
    "print(testPred)\n",
    "testTrue = trainY_cat[0]\n",
    "print(testTrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (8,) (7,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-6896927ad066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#margin_loss( trainY_cat[0], testPred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestTrue\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtestPred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtestTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestPred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (8,) (7,) "
     ]
    }
   ],
   "source": [
    "#margin_loss( trainY_cat[0], testPred)\n",
    "L = testTrue * np.square(np.maximum(0., 0.9 - testPred)) + 0.5 * (1 - testTrue) * np.square(np.maximum(0., testPred - 0.1))\n",
    "print(L)\n",
    "print(np.mean(np.sum(L, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When passing a list as loss, it should have one entry per model outputs. The model has 1 outputs, but you passed loss=[<function margin_loss at 0x7f527ba0b0d0>, 'mse']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-658814f759f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmargin_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam_recon\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m               metrics={'out_caps': 'accuracy'})\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m                                  \u001b[0;34m'The model has '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                                  \u001b[0;34m' outputs, but you passed loss='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                                  str(loss))\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mloss_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: When passing a list as loss, it should have one entry per model outputs. The model has 1 outputs, but you passed loss=[<function margin_loss at 0x7f527ba0b0d0>, 'mse']"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "capsmodel.compile(optimizer=opt, #'adam',\n",
    "              loss=[margin_loss, 'mse'],\n",
    "              loss_weights=[1., lam_recon],\n",
    "              metrics={'out_caps': 'accuracy'})\n",
    "\n",
    "# train the model\n",
    "data = capsmodel.fit( [trainX, trainY_cat], \n",
    "                      [trainY_cat, trainX], \n",
    "                      batch_size=batch_size, \n",
    "                      epochs=epochs, \n",
    "                      #validation_data=[[devX], [devY_cat]], \n",
    "                      validation_data=[[devX, devY_cat], [devY_cat, devX]], \n",
    "                      callbacks=[log, tb, checkpoint], \n",
    "                      verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX.shape\n",
    "trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(devX.shape)\n",
    "print(devY.shape)\n",
    "print(devY_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
