{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport keras\nimport theano","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"word_embeddings_path = '../input/glove-840b-300d/glove.840B.300d.txt'\nimport io\nword2idx = {}\nword_embeddings = []\nembedding_size = None\n#Loading embeddings\nwith io.open(word_embeddings_path, 'r', encoding=\"utf-8\") as f_em:\n    for line in f_em:\n        split = line.strip().split(\" \")\n        if len(split) <= 2:\n            continue\n        if embedding_size is None:\n            embedding_size = len(split) - 1\n            # Embeddings iniatilization for paddings and unknown words\n            word2idx[\"PADDING_TOKEN\"] = len(word2idx)\n            word_embeddings.append(np.zeros(embedding_size))\n\n            word2idx[\"UNKNOWN_TOKEN\"] = len(word2idx)\n            word_embeddings.append(np.random.uniform(-0.25, 0.25, embedding_size))\n        if len(split) - 1 != embedding_size:\n            continue\n        word_embeddings.append(np.asarray(split[1:], dtype='float32'))\n        word2idx[split[0]] = len(word2idx)\n\nword_embeddings = np.array(word_embeddings, dtype='float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6239b0a076ae29d74591443d0266a5f5139e7e77"},"cell_type":"code","source":"case2idx = {'numeric': 0, 'all_lower':1, 'all_upper':2, 'initial_upper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\ncase_embeddings = np.identity(len(case2idx), dtype=theano.config.floatX)\n\ndef get_casing(word, case_lookup):   \n    casing = 'other'\n    \n    num_digits = 0\n    for char in word:\n        if char.isdigit():\n            num_digits += 1\n            \n    digit_fraction = num_digits / float(len(word))\n    \n    if word.isdigit(): #Digit\n        casing = 'numeric'\n    elif digit_fraction > 0.5:\n        casing = 'mainly_numeric'\n    elif word.islower(): #All lower\n        casing = 'all_lower'\n    elif word.isupper(): #All upper\n        casing = 'all_upper'\n    elif word[0].isupper(): #First upper,other lower\n        casing = 'initial_upper'\n    elif num_digits > 0:\n        casing = 'contains_digit'  \n   \n    return case_lookup[casing]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a9b96ffbe5be1e2f550a7ea175da471be5d72ae"},"cell_type":"code","source":"MAX_COLUMNS = 2\nWORD_COL_NUM = 0\nLABEL_COL_NUM = 1\ndef read_file(file_path):\n    \"\"\"\n    :param file_path: path for corpus in CoNLL-format\n    :return: corpus_sentences - list of sentences, splitted into words\n    \"\"\"\n    corpus_sentences = []\n    input_sentence = []\n    with open(file_path, 'r', encoding='utf-8') as f_in:\n        for line in f_in:\n            line = line.strip()\n\n            if len(line) == 0 or line[0] == '#':\n                if len(input_sentence) > 0:\n                    corpus_sentences.append(input_sentence)\n                    input_sentence = []\n                continue\n            if len(line.split('\\t')) < MAX_COLUMNS:\n                print(line)\n                continue\n            input_sentence.append(line.split('\\t'))\n\n    if len(input_sentence) > 0:\n        corpus_sentences.append(input_sentence)\n\n    print(file_path, len(corpus_sentences), \"sentences\")\n    return corpus_sentences\n\n#Path for parts of CoNLL-2003 corpus\ntrain_path = '../input/conll2003/conll.train'\ntrain_sentences = read_file(train_path)\n\ndev_path = '../input/conll2003/conll.dev'\ndev_sentences = read_file(dev_path)\n\ntest_path = '../input/conll2003/conll.test'\ntest_sentences = read_file(test_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59b562e086bce400a2343069fcf8a231cc655ba7"},"cell_type":"code","source":"#Loading all class labels and adding new label for paddings\nlabel_set = set()\nlabel_set.add('PADDING_LABEL')\nfor dataset in [train_sentences, dev_sentences, test_sentences]:\n    for sentence in dataset:\n        for token in sentence:\n            label = token[LABEL_COL_NUM]\n            label_set.add(label)    \n\n# Turing labels into indices\nlabel2idx = {}\nidx2label = {}\nfor label in label_set:\n    label2idx[label] = len(label2idx)\n    \nprint(label2idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8e5f32cc6183b7bdf8c86597b84c91ee7346d0fc"},"cell_type":"code","source":"def get_token_indices(token, word2idx, case2idx, unknown_idx):\n\n    token_unknown = False\n    # Each token has several corresponding columns. Token text is in first column\n    word = token[WORD_COL_NUM]\n    # First trying to find word in embedding dictionary, if unable trying to find decapitalized word, if unable\n    # word is considered unknown\n    if word2idx.get(word) is not None:\n        word_idx = word2idx[word]\n    elif word2idx.get(word.lower()) is not None:\n        word_idx = word2idx[word.lower()]\n    else:\n        word_idx = unknown_idx\n        token_unknown = True\n\n    case_idx = get_casing(word, case2idx)\n    return token_unknown, word_idx, case_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d814d0664d9d2b68dcd23e9a72beea0d13a7458"},"cell_type":"code","source":"train_sentences[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9bb9ca6fa89d7415dac81700f0307e32957da52"},"cell_type":"code","source":"def create_matrices(sentences, word2idx, label2idx, case2idx):   \n    \n    unknown_idx = word2idx['UNKNOWN_TOKEN']\n    padding_casing = case2idx['PADDING_TOKEN']\n    padding_idx = word2idx['PADDING_TOKEN'] \n    padding_label = label2idx['PADDING_LABEL']  \n    \n    dataset = []\n    total_tokens = 0\n    unknown_tokens = 0\n    for sentence in sentences:\n        \n        # Index of first non-padding in sentence\n        proper_sentence_start = 1\n\n        word_indices = np.array([padding_idx] * (len(sentence) + 2))\n        case_indices = np.array([padding_casing] * (len(sentence) + 2))\n        label_indices = np.array([padding_label] * (len(sentence) + 2))\n\n        for pos_in_sentence, word in enumerate(sentence):\n\n            token_unknown, word_idx, case_idx = get_token_indices(word, word2idx, case2idx, unknown_idx)\n            pos_in_padded_sentence = pos_in_sentence + proper_sentence_start\n            word_indices[pos_in_padded_sentence] = word_idx\n            case_indices[pos_in_padded_sentence] = case_idx\n            label_indices[pos_in_padded_sentence] = label2idx[word[LABEL_COL_NUM]]\n\n            # Calculating percent of tokens not covered by embeddings\n            total_tokens += 1\n            if token_unknown:\n                unknown_tokens += 1\n\n        # All data for one sentence put in one list\n        dataset.append([word_indices, case_indices, label_indices])\n        \n    percent = 0.0\n    if total_tokens != 0:\n        percent = float(unknown_tokens) / total_tokens * 100\n    print(\"{} tokens, {} unknown, {:.3}%\".format(total_tokens, unknown_tokens, percent ))\n    return dataset\n\n\n\ntrain_data = create_matrices(train_sentences, word2idx, label2idx, case2idx)\ndev_data = create_matrices(dev_sentences, word2idx, label2idx, case2idx)\ntest_data = create_matrices(test_sentences, word2idx, label2idx, case2idx)\n\nfor sentence in train_data[:5]:\n    print(sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3f95656af9c06c46dd3d86740a6beb1d9cfe481"},"cell_type":"code","source":"from keras.layers import Embedding, LSTM, Dense, TimeDistributed, Dropout, Bidirectional, Input, concatenate\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n# Practical implementations should have dim of 100 or more\nSENTENCE_LSTM_DIM = 10\n\nn_out = len(label2idx)\n\ntokens_input = Input(dtype='int32', shape=(None,), name='tokens_input')\ntokens_embedding_layer = Embedding(input_dim=word_embeddings.shape[0], \n                                   output_dim=word_embeddings.shape[1],\n                                   weights=[word_embeddings], trainable=False, \n                                   name='tokens_embeddings')\ntokens = tokens_embedding_layer(tokens_input)\n\n\ncasing_input = Input(dtype='int32', shape=(None,), name='casing_input')\ncasing_embedding_layer = Embedding(input_dim=case_embeddings.shape[0], \n                                   output_dim=case_embeddings.shape[1],\n                                   weights=[case_embeddings], trainable=True, \n                                   name='casing_embeddings')\ncasing = casing_embedding_layer(casing_input)\n\nmerged_embeddings = concatenate([tokens, casing], name='merged_embeddings')\nfor_lstm = Dropout(0.2)(merged_embeddings)\n# If GPU is used  choose implementation=2\nblstm = Bidirectional(LSTM(SENTENCE_LSTM_DIM, return_sequences=True, implementation=1), \n                      name='blstm')(for_lstm)\n\nresult = TimeDistributed(Dense(n_out, activation='softmax', name='result'))(blstm)\n\nmodel = Model(inputs=[tokens_input, casing_input], outputs=result)\n\n# default lr = 0.001, beta_1=0.9\nadam = Adam(lr=0.001, beta_1=0.9)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=adam)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16ddcc3363da65b34167f366f11d236589c61009"},"cell_type":"code","source":"import random\nimport time\n\ndef iterate_minibatches(dataset):   \n    for sentence in dataset:\n        tokens, casing, labels = sentence     \n            \n        labels = np.expand_dims(labels, -1) \n        yield np.asarray([tokens]), np.asarray([casing]), np.asarray([labels])\n\n# Here again code should be adapted for batches of sentences     \ndef tag_dataset(dataset):\n    predicted_labels = []\n    correct_labels = []\n    for tokens, casing, labels in dataset:\n        pred = model.predict_on_batch([np.asarray([tokens]), np.asarray([casing])])[0]\n        pred_labels = [el.tolist().index(max(el)) for el in pred]\n        predicted_labels.append(pred_labels)\n        correct_labels.append(labels)\n        #print(predicted_labels, correct_labels)\n    return predicted_labels, correct_labels\n\ndef compute_accuracy(predictions, correct, padding_label):\n    total_tokens = 0\n    guessed_tokens = 0\n    for guessed_sentence, correct_sentence in zip(predictions, correct):\n        assert (len(guessed_sentence) == len(correct_sentence)), \"Guessed and correct sentences do not match\"\n        for j in range(len(guessed_sentence)):\n            if correct_sentence[j] != padding_label:\n                total_tokens += 1\n                if guessed_sentence[j] == correct_sentence[j]:\n                    guessed_tokens += 1\n\n    if total_tokens == 0:\n        return float(0)\n    else:\n        accuracy = float(guessed_tokens) / total_tokens\n        return accuracy\n\n        \nnumber_of_epochs = 10\nprint(\"%d epochs\" % number_of_epochs)\n\nprint(\"%d train sentences\" % len(train_data))\nprint(\"%d dev sentences\" % len(dev_data))\nprint(\"%d test sentences\" % len(test_data))\n\npadding_label = label2idx['PADDING_LABEL']\n\nfor epoch in range(number_of_epochs):    \n    print(\"--------- Epoch %d -----------\" % epoch)\n    random.shuffle(train_data)\n    \n    start_time = time.time()    \n    for batch in iterate_minibatches(train_data):\n        tokens, casing, labels = batch       \n        model.train_on_batch([tokens, casing], labels)   \n    print(\"%.2f sec for training\" % (time.time() - start_time))\n               \n    #Train Dataset       \n    start_time = time.time()  \n    print(\"================================== Train Data ==================================\")\n    predicted_labels, correct_labels = tag_dataset(train_data)        \n    accuracy = compute_accuracy(predicted_labels, correct_labels, padding_label)\n    print(\"Accuracy = \", accuracy)\n\n    #Dev Dataset \n    print(\"================================== Dev Data: ==================================\")\n    predicted_labels, correct_labels = tag_dataset(dev_data)  \n    accuracy = compute_accuracy(predicted_labels, correct_labels, padding_label)\n    print(\"Accuracy = \", accuracy)\n\n\n    #Test Dataset \n    print(\"================================== Test Data: ==================================\")\n    predicted_labels, correct_labels = tag_dataset(test_data)  \n    accuracy = compute_accuracy(predicted_labels, correct_labels, padding_label)\n    print(\"Accuracy = \", accuracy)\n\n        \n    print(\"%.2f sec for evaluation\" % (time.time() - start_time))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}