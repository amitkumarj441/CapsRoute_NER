{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten, PReLU\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\n\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.utils.vis_utils import plot_model","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', sep=\"\\t\")\ntest = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', sep=\"\\t\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"82e461db3555ca25efca6e6da3dc557501a484ab"},"cell_type":"code","source":"tk = Tokenizer(lower = True, filters='')\nfull_text = list(train['Phrase'].values) + list(test['Phrase'].values)\ntk.fit_on_texts(full_text)\ntrain_tokenized = tk.texts_to_sequences(train['Phrase'])\ntest_tokenized = tk.texts_to_sequences(test['Phrase'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53484cd399f639726b99575e7f661d18aae5342a","collapsed":true},"cell_type":"code","source":"max_len = 200\nX_train = pad_sequences(train_tokenized, maxlen = max_len)\nX_test = pad_sequences(test_tokenized, maxlen = max_len)\n\nembedding_path = \"../input/fasttext-toxic/crawl-300d-2M.vec\" #toxic-fasttext from jigsaw \n\nembed_size = 300\nmax_features = 100000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1286c841611d3d5fda24fa692be1d1fa69abddb","collapsed":true},"cell_type":"code","source":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n\nword_index = tk.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words + 1, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05895af1ff1d63f31c50e11df808e89e2f05bd6f","collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ny = train['Sentiment']\nohe = OneHotEncoder(sparse=False)\ny_ohe = ohe.fit_transform(y.values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1a9d1b7f00f01c842d2abe3293f97bbd6f846a6","collapsed":true},"cell_type":"code","source":"file_path = \"best_model.hdf5\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1034ae130f9d1b6f8c59e072db4442a438aad6f0","collapsed":true},"cell_type":"code","source":"def squash(x, axis=-1):\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n    scale = K.sqrt(s_squared_norm + K.epsilon())\n    return x / scale","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"78782c47dd6105f1191e4f9bb3e38c8aeffa19c2"},"cell_type":"code","source":"# A Capsule Implement with Pure Keras\nclass Capsule(Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n                 activation='default', **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.kernel_size = kernel_size\n        self.share_weights = share_weights\n        if activation == 'default':\n            self.activation = squash\n        else:\n            self.activation = Activation(activation)\n\n    def build(self, input_shape):\n        super(Capsule, self).build(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1, input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     # shape=self.kernel_size,\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n\n    def call(self, u_vecs):\n        if self.share_weights:\n            u_hat_vecs = K.conv1d(u_vecs, self.W)\n        else:\n            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n        batch_size = K.shape(u_vecs)[0]\n        input_num_capsule = K.shape(u_vecs)[1]\n        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n                                            self.num_capsule, self.dim_capsule))\n        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n\n        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  \n        for i in range(self.routings):\n            b = K.permute_dimensions(b, (0, 2, 1))  \n            c = K.softmax(b)\n            c = K.permute_dimensions(c, (0, 2, 1))\n            b = K.permute_dimensions(b, (0, 2, 1))\n            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n            if i < self.routings - 1:\n                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0289082032d2125a663427410b4bcffd9203a5d7"},"cell_type":"code","source":"def get_model_capsule(gru_spec=(128, 0), gru_dropout=5e-5, lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n    inp = Input(shape = (max_len,))\n    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x1 = SpatialDropout1D(dr)(x)\n\n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n    x_words = PReLU()(x_gru)\n    x_words = Capsule(num_capsule=5, dim_capsule=16, routings=5, share_weights=True)(x_words)\n    x_words = Flatten()(x_words)\n    x_words = Dropout(0.15)(x_words)\n\n    \n    x = Dropout(0.1)(Dense(64,activation='relu') (x_words))\n    pred = Dense(5, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = pred)\n    model.summary()\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n                        verbose = 1, callbacks = [check_point, early_stop])\n\n    return model\n    try:\n        from keras.utils.vis_utils import plot_model as plot\n        plot(model, to_file='model.png', show_shapes=True)\n    except Exception as e:\n        print('No fancy plot {}'.format(e))\n\nmodel_capsule = get_model_capsule(lr = 1e-3, units = 128, dr = 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"093cee010a523f0d99f9e44eb3ef9afa6ff07e69"},"cell_type":"code","source":"pred = model_capsule.predict(X_test, batch_size = 1024, verbose = 1)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}